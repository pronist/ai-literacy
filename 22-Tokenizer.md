# 토크나이저

> GPT와 같은 모델 이용료를 지불하는 기준은 토큰(Token)의 사용량입니다. 일반적으로 입력 토큰(프롬프트)보다 출력 토큰(응답)에 더 높은 가치를 부여하여 이용료가 비쌉니다. 즉, 모델의 답변이 길면 길수록 토큰을 더 많이 소비하여 비용이 많이 든다는 뜻입니다. 한글의 경우 입출력 토큰이 더 많이 요구되므로 영어에 비해 비쌉니다.

> 한글는 영어보다 토큰 수가 많아질 수 있으므로, 긴 답변이 필요한 경우 요약 또는 출력 길이 제한을 설정하는 것이 좋습니다.

* OpenAI → API → Pricing → https://openai.com/api/pricing
* OpenAI Tokenizer → https://platform.openai.com/tokenizer

## 개요

- 언어모델이 이해할 수 있는 표현을 벡터(Vector)라고 하는데, 벡터로 변환하기 전에 토큰(Token)이라는 단위로 자연어를 먼저 쪼개야 합니다. 
- 자연어를 토큰으로 쪼개는 것을 토큰화(Tokenize)라고 하며, 이 역할을 하는 것이 토크나이저(Tokenizer)입니다. 
- 토크나이저에 의해 분리된 자연어에는 숫자 형태의 정수 인덱스(Integer Index)가 부여됩니다.

```
┌───────────────────────────────┐
│         [입력 프롬프트]
│     "안녕하세요, 오늘 날씨는?"
└───────────────────────────────┘
                ↓
┌───────────────────────────────┐
│          [토크나이저]
│      "안", "녕", "하", ...
│   → [101, 345, 876, ...]
└───────────────────────────────┘
                ↓
┌───────────────────────────────┐
│          [트랜스포머]
└───────────────────────────────┘
```

- 토크나이저는 자연어 문장을 단어, 서브워드와 같은 "사람이 이해하는 언어 단위" 기준으로 문장을 쪼갭니다.
- 토큰을 분리하는 방식과 기준은 토크나이저마다 다를 수 있으며 영어처럼 띄어쓰기가 명확한 언어는 처리하기 비교적 쉽습니다.
- 조합형 문자인 한글의 경우 더 복잡하여 영어보다 더 많은 토큰으로 분리되는 경향이 있습니다. (토크나이저가 과거의 것일수록 한글이 더 많은 토큰을 소모합니다.)

> 하나의 단어가 하나의 토큰으로 나뉘는 것이 아니라, 토큰은 단어보다 작거나 큰 단위일 수 있습니다.

## 종류

| 모델 | 토크나이저 방식 | 특징 |
|------|------------------|------|
| GPT | BPE(Byte Pair Encoding) | 글자 조합 기반, 효율적 |
| BERT | WordPiece | 어절/형태소 조각으로 나눔 |
| T5 | SentencePiece | 공백 없는 언어에 강함 (한글 등) |

## 컨텍스트 윈도우

- 컨텍스트 윈도우(Context Window)는 모델이 집중해서 읽을 수 있는 한 화면의 크기 같은 개념으로, 언어모델이 이해할 수 있는 최대 토큰 수를 의미합니다. 
- 예를 들어 GPT-3.5는 약 4,000 토큰, GPT-4는 8,000~128,000 토큰까지 한 번에 받아들일 수 있습니다.
- 컨텍스트 윈도우가 클수록 언어모델이 더 많은 대량의 문장도 한번에 이해할 수 있다는 의미로, 복잡한 요청, 문서 요약, 긴 대화 유지 등에 유리합니다. 
- 컨텍스트 윈도우를 넘어서는 내용을 입력하면 무시하거나 생략하고, 시스템 프롬프트도 손상될 수 있습니다.

> 윈도우 한계를 넘어서면 맥락 주입을 통해 흐름을 유지할 수 있습니다.

```
[GPT-4 기준 컨텍스트 윈도우: 8192 토큰]

┌───────────────┐ 
│ 시스템 프롬프트 500토큰 
│ 문서 입력(RAG) 1500토큰 
│ 사용자 프롬프트 800토큰 
└───────────────┘
= 총 입력: 2800 토큰 → 출력 가능 범위: 5392 토큰
```

### vs 메모리

- 컨텍스트 윈도우는 세션 내 기억, 메모리는 세션 밖 기억입니다.

| 구분 | 컨텍스트 윈도우 | 메모리 |
| --- | --- | --- |
| 위치 | 대화 내 입력/출력 범위 | 대화 외부 시스템 저장소 |
| 지속성 | 현재 대화 안에서만 유지 | 여러 대화 간 유지 가능 |
| 자동성 | 모두 자동 적용 | 설정에 따라 자동/수동 선택 |
| 사용 예 | 최근 질문 이해 | 어조, 말투, 선호 내용 반영 |

## 정리

| 항목 | 설명 | 예시 |
| --- | --- | --- |
| 정의 | 자연어 문장을 토큰(Token) 단위로 쪼개고 숫자 인덱스로 변환 | "안녕하세요" → "안", "녕", "하", … → [101, 345, 876, …] |
| 비용과 관계 | 토큰 수 기준으로 이용 요금 발생 (출력이 많을수록 비용 증가) | 한글은 영어보다 토큰 수가 많아질 수 있음 |
| 컨텍스트 윈도우 | 모델이 한 번에 읽을 수 있는 최대 토큰 수 | GPT-3.5: 약 4,000 / GPT-4: 8,000~128,000 토큰 |
| vs 메모리 | 컨텍스트 윈도우: 세션 내 기억 / 메모리: 세션 외 기억 | 대화 안: 윈도우 / 대화 밖: 메모리 |

## 요약

- 토크나이저는 자연어를 토큰 단위로 쪼개어 모델이 이해할 수 있는 숫자 형태로 변환하는 기능입니다.
- 토큰 수에 따라 비용이 발생하며, 출력 길이와 컨텍스트 윈도우 크기가 중요한 요소입니다.

## 생각해보기

- 한글과 영어의 토큰 수 차이가 나는 이유는 무엇인가요?
- 토크나이저가 없다면 언어모델이 자연어를 이해하기 어려운 이유는 무엇인가요?
- 컨텍스트 윈도우가 작은 모델과 큰 모델은 어떤 차이가 있나요?
- 토크나이저 종류(GPT, BERT, T5)마다 방식이 다른 이유는 무엇인가요?
