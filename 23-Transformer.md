# 트랜스포머

> 생성형 AI 서비스 중 대중적으로 가장 널리 알려진 ChatGPT에서 사용되는 모델인 GPT는 풀어서 쓰면 Generative Pre-trained Transformer 입니다. Generative(생성형), Pre-trained(사전 학습된), 까지는 알겠는데, 트랜스포머(Transformer)는 무엇일까요?

## 개요

- 2017년에 구글이 제시한 [Attention is All You Need](https://arxiv.org/pdf/1706.03762)라는 논문에서 소개되었습니다. 
- 트랜스포머는 우리가 알고 있는 대부분의 언어모델에 사용되고 있는 딥러닝(Deep Learning) 모델 아키텍처입니다. 
- 딥러닝은 기계를 학습시키기 위한 머신러닝의 일종으로, 신경망(Neural Network)이라는 구조를 사용하여 사람의 뇌를 흉내 내서 컴퓨터가 스스로 배우고 판단하게 만드는 기술입니다. 
- 사람이 학습하듯이 복잡한 구조를 보고 패턴을 인식하여 체계화하거나 '인간의' 표현 방식을 배웁니다.
- 트랜스포머는 순차적으로 단어를 하나씩 처리하던 과거와는 달리 문장 전체를 동시(병렬적으로)에 분석합니다.  
- 문장을 이해하는 속도가 훨씬 빠르고, 긴 문장에서도 멀리 떨어진 단어들의 관계를 잘 이해할 수 있습니다. 

## 임베딩

- 임베딩(Embedding)은 정수 인덱스가 부여된 토큰을 "언어모델이 처리할 수 있는" 형태인 벡터(Vector)로 변환하는 역할을 합니다. 
- 임베딩을 통해 벡터로 바뀐 자연어는 숫자 배열로 구성되며, 다차원 공간에서 표현될 수 있습니다. 
- 내부적으로 유사도를 계산(내적; Scaled Dot Product)하여 다른 표현이지만 비슷한 의미를 가진 말들을 이해할 수 있게 됩니다.

```
┌───────────────────────────────┐ 
│         [입력 프롬프트]
│    "안녕하세요, 오늘 날씨는?"   
└───────────────────────────────┘
               ↓
┌───────────────────────────────┐
│          [토크나이저]         
│      "안", "녕", "하", ...       
│  → [101, 345, 876, ...]      
└───────────────────────────────┘
               ↓
┌───────────────────────────────┐
│           [임베딩]       
│  정수 인덱스를 벡터로 변환     
│  → [[0.12, -0.5, ...], ...]   
└───────────────────────────────┘
```

## 포지셔널 인코딩

- 트랜스포머는 단어의 순서를 기억하지 않기 때문에, 포지셔널 인코딩(Positional Encoding)을 통해 위치 정보를 별도로 제공합니다.
- 포지셔널 인코딩은 위치 정보를 가진 벡터로, 임베딩 벡터와 더해짐으로써 단어의 '순서'를 인식할 수 있게 해줍니다.

```
┌───────────────────────────────┐
│           [임베딩]
│       [0.13, 0.89, ...]
└───────────────────────────────┘
               ↓
┌───────────────────────────────┐
│        [포지셔널 인코딩]
│       [0.01, -0.34, ...] 
└───────────────────────────────┘
               ↓
┌───────────────────────────────┐
│    [합쳐진 벡터 = 입력 벡터]
│       [0.14, 0.55, ...]
└───────────────────────────────┘
```

## 디코더

- 자연어 프롬프트가 입력되면, 토크나이저를 거쳐 토큰 ID로 변환되고 임베딩과 포지셔널 인코딩을 통해 벡터화된 후 디코더의 입력으로 처리됩니다. 
- 디코더는 현재까지 생성된 토큰 시퀀스를 받아, 다음 토큰을 예측하는 방식으로 작동합니다. 이 과정을 반복하며 문장을 만들어냅니다.
- 벡터 → Self-Attention → 확률 분포 → 샘플링 → 출력 과정을 반복합니다. 이 과정을 자기회귀(AR; Autoregressive Language Modeling)라 부릅니다. 
- 종료(Termination) 신호가 나오거나, 정해진 최대 길이(Max Tokens)에 도달하면 답변 생성을 멈춥니다.

> GPT는 인코더 없이 디코더만 사용하는 구조입니다.

> 우리가 설계하는 프롬프트는 결국 디코더의 입력으로 들어가며, 이 입력이 문맥적으로 어떻게 해석될지를 결정짓습니다. 따라서 어떻게 문장을 구성하느냐가 매우 중요합니다.

```
┌───────────────────────────────┐
│    [합쳐진 벡터 = 입력 벡터]
│       [0.14, 0.55, ...]
└───────────────────────────────┘
               ↓
┌───────────────────────────────┐
│       [디코더 블록들 × N]         
│ - Self Attention + Masking   
│ 여러 층으로 반복됨            
└───────────────────────────────┘
               ↓
┌───────────────────────────────┐
│           [로짓 벡터]          
│ 어휘 집합의 각 단어에 대한 '점수'     
│ 예: "맑"=3.5, "눈"=2.1, ...    
└───────────────────────────────┘
               ↓
┌───────────────────────────────┐
│           [Softmax]       
│ → Softmax로 확률 분포 생성 ← 단어가 선택될 확률을 계산하는 수학 도구            
└───────────────────────────────┘
               ↓
┌───────────────────────────────┐
│            [샘플링]       
│ → 샘플링 전략(Top-k 등)으로 토큰 선택
│ 예: "맑" 선택됨                  
└───────────────────────────────┘
               ↓
┌───────────────────────────────┐
│          [디토크나이징]         
│ → 토큰 → 문자 조합           
│ 예: "맑", "고", "개", "임"     
│ → "맑고 개임"                 
└───────────────────────────────┘
```

## GPT(디코더-only) 트랜스포머 구조

- 이 과정을 통해 GPT는 입력된 프롬프트를 차례로 분석하고, 다음 단어를 순차적으로 예측하여 문장을 생성합니다.

```
┌───────────────────────────────┐
│         [입력 프롬프트]            
│    "안녕하세요, 오늘 날씨는?"             
└───────────────────────────────┘
                ↓
┌───────────────────────────────┐
│          [토크나이저]                   
│ "안", "녕", "하", ... → [101, 345, 876, ...] 
└───────────────────────────────┘
                ↓
┌───────────────────────────────────────┐
│ 트랜스포머(Transformer)
│                                            
│ ┌───────────────────────────────┐    
│ │            [임베딩] 
│ │  토큰 ID → 벡터 변환              
│ │  예: "안" → [0.12, -0.5, ...]     
│ └───────────────────────────────┘    
│                   ↓                        
│ ┌───────────────────────────────┐    
│ │         [포지셔널 인코딩]           
│ │ 단어 순서 정보 추가 → 임베딩 + 위치 벡터     
│ └───────────────────────────────┘    
│                   ↓                        
│ ┌───────────────────────────────┐    
│ │        [디코더 블록들 × N]        
│ │  - Self Attention + Masking         
│ │  - 여러 층으로 반복됨  
│ └───────────────────────────────┘   
│                  ↓
│ ┌───────────────────────────────┐
│ │           [로짓 벡터]          
│ │ 어휘 집합의 각 단어에 대한 '점수'    
│ │ 예: "맑"=3.5, "눈"=2.1, ...    
│ └───────────────────────────────┘
│                  ↓
│ ┌───────────────────────────────┐
│ │           [Softmax]       
│ │ → Softmax로 확률 분포 생성  
│ └───────────────────────────────┘ 
└───────────────────────────────────────┘
                ↓
┌───────────────────────────────┐
│            [샘플링]       
│ → 샘플링 전략(Top-k 등)으로 토큰 선택
│ 예: "맑" 선택됨                  
└───────────────────────────────┘
                ↓
┌───────────────────────────────┐
│          [디토크나이징]
│ 토큰 ID → 문자로 변환
│ 예: "맑", "고", "개", "임" → "맑고 개임"
└───────────────────────────────┘
```

## 정리

| 항목 | 설명 | 예시 |
| --- | --- | --- |
| 정의 | 2017년 구글이 제안한 'Attention is All You Need' 논문 기반 딥러닝 모델 구조 | GPT, BERT, T5 등 주요 언어모델의 기반 구조 |
| 특징 | 순차 처리(X), 병렬 처리(O) → 긴 문장도 빠르고 정확하게 문맥 이해 | 앞뒤 단어의 관계를 모두 고려해 이해 |
| 핵심 구성 요소 | 임베딩, 포지셔널 인코딩, 어텐션(Self-Attention), 디코더 | 임베딩: 벡터화 / 포지셔널 인코딩: 순서 정보 / Self-Attention: 관계 계산 |
| 디코더 | 입력을 받아 다음 단어를 예측하며 출력 생성 | "오늘 날씨는…" → "맑아요" 예측 |

## 요약

- 트랜스포머는 병렬 처리와 Self-Attention을 통해 문장의 문맥을 효율적으로 이해하는 모델 구조입니다.
- 임베딩, 포지셔널 인코딩, 어텐션, 디코더 등으로 구성되며, GPT는 디코더만 사용합니다.

## 생각해보기

- 트랜스포머가 순차적으로 처리하던 방식보다 뛰어난 점은 무엇인가요?
- 포지셔널 인코딩은 왜 필요한가요?
