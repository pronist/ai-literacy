# 어텐션

- 트랜스포머는 어텐션(Attention)이라는 기술을 사용하여 단어, 문장, 문단 간의 맥락과 관계를 파악합니다. 

## Self-Attention

- Self-Attention은 문장 내의 모든 단어들이 서로를 바라보며, 각 단어 간의 관계를 계산해 맥락 정보를 반영한 벡터를 생성하는 구조입니다. 
- 입력된 각 단어(토큰)를 대상으로 `Query`, `Key`, `Value` 벡터를 계산합니다. 
- 각 단어는 `Query`와 다른 단어의 `Key`간 유사도를 계산(Scaled Dot Product → 두 벡터의 방향성과 크기를 곱하는 계산)합니다.

> GPT처럼 디코더 기반 모델에서는 다음 단어 예측이 주 목적이지만, Self-Attention 자체의 정의는 관계 계산이 핵심입니다.

| 요소 | 역할 | 비유 |
|------|------|------|
| Query | 어떤 정보를 찾고 싶은 기준 | 사용자의 검색 요청 |
| Key | 각 단어가 가진 태그 정보 | 책의 주제 태그 |
| Value | 토큰에 대한 정보 벡터 | 책의 핵심 요약 |

> 도서관에서 '고양이'라는 주제(Q)를 찾을 때, 책장(Key)에 붙은 주제 태그를 보고 관련 내용을 찾고(Value) 읽는 것과 같습니다.

### 원리

```
┌───────────────────────────────┐
│         [입력 프롬프트]
│ "고양이" "는" "침대" "에서" "잔다"
└───────────────────────────────┘
                ↓                        
┌───────────────────────────────┐
│       [Query/Key/Value]      
│ "고양이" → Q1, K1, V1                  
│ "는"     → Q2, K2, V2                  
│ "침대"   → Q3, K3, V3                  
│ "에서"   → Q4, K4, V4                  
│ "잔다"   → Q5, K5, V5                  
└───────────────────────────────┘
                ↓                        
┌───────────────────────────────┐
│    [Query · Key 유사도 계산]                                                   
│ Q1 ⋅ K1 →  10.5   ← "고양이"와 "고양이"           
│ Q1 ⋅ K2 →   2.3   ← "고양이"와 "는"               
│ Q1 ⋅ K3 →   6.1   ← "고양이"와 "침대"             
│ Q1 ⋅ K4 →   4.4   ← "고양이"와 "에서"             
│ Q1 ⋅ K5 →   9.7   ← "고양이"와 "잔다"             
└───────────────────────────────┘
                ↓
┌───────────────────────────────┐
│           [Softmax]
│ → 어텐션 스코어 (확률 분포) 변환      
└───────────────────────────────┘
                ↓
        Q1 기준 주의 집중
┌───────────────────────────────┐
│         [어텐션 스코어]                          
│ 고양이 → 고양이: (0.32)             
│ 고양이 → 는: (0.02)             
│ 고양이 → 침대: (0.12)             
│ 고양이 → 에서: (0.06)             
│ 고양이 → 잔다: (0.48) ← 가장 집중                   
└───────────────────────────────┘
               ↓
┌───────────────────────────────┐
│         [어텐션 가중합]
| (각 단어의 Value 벡터 × 어텐션 스코어)
|
| Context 벡터 = ← 다음 연산의 입력
| 0.32 × V1 + 
| 0.02 × V2 +
| 0.12 × V3 + 
| 0.06 × V4 +
| 0.48 × V5
└───────────────────────────────┘
               ↓
┌───────────────────────────────┐
│       다음 레이어로 전달            
└───────────────────────────────┘
```

- '한 단어가 다른 단어들과 얼마나 관련 있는지를 계산해, 가중합으로 새로운 벡터를 만드는 과정'은 Self-Attention의 일반적인 메커니즘이고, 인코더/디코더 모두에서 발생합니다. 
- 단, 인코더의 Self-Attention은 입력 전체를 동시에 보며 학습하고, 디코더의 Self-Attention은 마스킹(Masking)하여 생성 중인 단어까지만 참고합니다.

| 구분 | 인코더 | 디코더 |
| --- | --- | --- |
| Self-Attention | 사용됨 (모든 토큰 간 관계 계산) | 사용됨 (과거 토큰까지만 사용) |
| 어텐션 범위 | 전체 문장 (Look-around) | 왼쪽만 (Look-ahead) ← 마스킹 |
| Masking | 없음 (전체 토큰 볼 수 있음) | 있음 (미래 토큰 가림) |
| 목적 | 입력 문장의 문맥 파악 | 응답 생성 중 문맥 유지 |

> Self-Attention은 시퀀스 길이가 길어질수록 계산이 복잡해질 뿐만 아니라 어텐션 스코어 매트릭스의 크기가 O(N^2)으로 증가하여 GPU 메모리 사용량이 많습니다. 이 문제를 해결하기 위해 Flash Attention이라는 어텐션을 사용하기도 합니다. Flash Attention을 사용하면 메모리 효율성이 증가하고, 추론 및 학습 속도가 개선됩니다. 

## Masking

- 마스킹(Masking)은 트랜스포머 모델의 Self-Attention에서 사용되는 개념으로, 모델이 특정 단어나 토큰을 '보지 못하게' 막는 역할을 합니다. 
- GPT와 같은 생성형 AI의 '훈련' 과정에서 다음에 올 단어를 예측(CLM; Causal Language Modeling)할 때 중요한 역할을 합니다. 

> 질의/응답 데이터를 기반으로 하는 지도 학습(Supervised Learning)과 문장 그 자체를 대상으로 학습하는 자기지도 학습(Self-Supervised Learning)을 할 때에는 언어모델은 이미 '정답'에 해당하는 문장을 알고 있습니다. 하지만 훈련 중에는 현재 출력한 토큰 이후의 문장은 가리고(Look-Ahead Masking), 다음에 올 토큰이 무엇인지 예측하는 방식으로 훈련이 진행됩니다.

```
입력 시퀀스:
[ A ]  [ B ]  [ C ]  [ D ]  [ E ]

Self-Attention 시 마스킹 처리 (Look-Ahead Mask)

         A    B    C    D    E
       ┌────┬────┬────┬────┬────┐
    A  │ ○  │ ×  │ ×  │ ×  │ ×
       ├────┼────┼────┼────┼────┤
    B  │ ○  │ ○  │ ×  │ ×  │ ×
       ├────┼────┼────┼────┼────┤
    C  │ ○  │ ○  │ ○  │ ×  │ ×
       ├────┼────┼────┼────┼────┤
    D  │ ○  │ ○  │ ○  │ ○  │ ×
       ├────┼────┼────┼────┼────┤
    E  │ ○  │ ○  │ ○  │ ○  │ ○
       └────┴────┴────┴────┴────┘
```

- GPT의 경우 추론에서 미래 토큰이 없기에 마스킹을 적용할 대상 자체가 없긴 하지만, 모델에서 마스킹은 구조 유지를 위해 사용됩니다.

| 모델 | 훈련 | 추론 | 종류 |
| --- | --- | --- | --- |
| BERT | 사용함 | 사용하지 않음 | MLM (일부 토큰 가림, 양방향) |
| GPT | 사용함 | 사용함 | Causal Masking (이후 토큰 가림, 단방향) |

### Masked Self-Attention

- Masked Self-Attention은 Self-Attention + Masking을 처리한 어텐션입니다. 
- 디코더에서 진행되며 마스킹을 사용하여 현재 토큰을 포함한 이전 토큰까지만 볼 수 있고 미래 토큰은 볼 수 없습니다.
- '이전까지 출력된 단어들'간의 관계와 맥락을 파악하고 그 다음에 나올 토큰을 예측합니다.

> 시작 토큰인 `<BOS>`(Beginning of Sentence)는 필수는 아니지만, 대부분의 트랜스포머 기반 디코더에서는 일반적으로 사용됩니다.

#### 원리

```
┌───────────────────────────────┐
│         [디코더 입력]         
│    "<BOS>" "고양이" "는"
└───────────────────────────────┘
               ↓
┌───────────────────────────────┐
│       [Query/Key/Value]      
│ "<BOS>"   → Q1, K1, V1                 
│ "고양이"  → Q2, K2, V2                 
│ "는"      → Q3, K3, V3                      
└───────────────────────────────┘
               ↓
┌───────────────────────────────┐
│    [Query · Key 유사도 계산]  
│ Q4 ⋅ K1 →  5.1   ← Q4와 "<BOS>"
│ Q4 ⋅ K2 →  7.8   ← Q4와 "고양이"           
│ Q4 ⋅ K3 → 11.0   ← Q4와 "는" 
│ → K4, V4, Q5, K5, ...는 마스킹되어 유사도 계산에서 제외됨
└───────────────────────────────┘
               ↓
┌───────────────────────────────┐
│           [Softmax]
│ → 어텐션 스코어 (확률 분포) 변환      
└───────────────────────────────┘
               ↓
        Q4 기준 주의 집중
┌───────────────────────────────┐
│         [어텐션 스코어]          
│ Q4 → <BOS>: (0.12)               
│ Q4 → 고양이: (0.27)             
│ Q4 → 는: (0.41)                
└───────────────────────────────┘
               ↓
┌───────────────────────────────┐
│         [어텐션 가중합]       
│ (각 Value × 어텐션 스코어의 합)       
│                                   
│ Context 벡터 = ← Q4의 문맥 기반 벡터
│ 0.12 × V1 +                    
│ 0.27 × V2 +                    
│ 0.41 × V3                      
└───────────────────────────────┘
               ↓
┌───────────────────────────────┐
│       다음 레이어로 전달
└───────────────────────────────┘
```

#### Q4 

- 디코더는 `<BOS>`, "고양이", "는"을 입력으로 받아, 현재까지 생성된 단어들만을 가지고 내부 상태를 계산합니다.
- 그 계산을 통해, 다음 단어를 예측하려는 위치에서 나오는 Query 벡터가 Q4입니다.
- Q4는 "다음 단어를 생성할 차례"를 나타내는 추상적 벡터입니다.
- 훈련 중에는 정답 토큰이 이미 존재하므로 디코더는 `<BOS>`, "고양이", "는"을 입력받고 "침대"를 예측합니다. 
이때 내부적으로 4번째 위치에 해당하는 Q4를 만들 수 있습니다.
- 추론 시에는 아직 "침대"를 생성하지 않았지만, 이전까지의 토큰만을 기반으로 다음 위치에 대한 Query(Q4)를 만듭니다.
이게 바로 Masked Self-Attention이 가능한 이유입니다.

> Q4는 아직 말하지 않았지만, "지금까지 말한 걸 바탕으로, 다음에 무슨 말을 해야 할까?" 하고 생각하는 순간의 내부 상태 벡터입니다. 이 상태(Q4)가 앞의 말들(K1~K3)과 얼마나 관련 있는지를 계산합니다.

## Multi-Head Attention

- 멀티헤드 어텐션(Multi-Head Attention)은 입력을 여러 개의 서로 다른 어텐션 헤드로 나눠 처리하는 방식입니다.
- 각 헤드는 입력 임베딩에 대해 서로 다른 가중치를 적용해 독립적인 Query, Key, Value를 생성하며, 각기 다른 방식으로 주의를 계산합니다.
- 멀티헤드가 많을수록 계산량은 늘어나지만, 다양한 관점에서 정보를 종합할 수 있어 더욱 정교한 문맥 이해가 가능합니다.
- 실제로 각 Head는 학습을 통해 다양한 역할을 자율적으로 분화하지만, 개념적으로는 문법, 위치, 의미 등 다양한 측면을 분리해 처리한다고 볼 수 있습니다.

> 계산된 헤드들의 출력을 결합(Concat)한 후에는 선형 변환을 거쳐 하나의 출력 벡터로 정리됩니다. 이를 통해 모델은 다양한 문맥적 관점에서 정보를 이해하고 통합할 수 있습니다. (예: 문법, 위치, 의미 등)

```
┌────────────────────────────┐
│         [입력 시퀀스]         
│ "고양이" "는" "침대" "에서" "잔다"    
└────────────────────────────┘
               ↓
┌────────────────────────────┐
│    [Self-Attention × h개]  ← 서로 다른 관점 (head)으로 수행
│ Head 1: 형태소 중심
│ Head 2: 위치 중심
│ Head 3: 문법 중심
│ ...
└────────────────────────────┘
               ↓
┌────────────────────────────┐
│          [Concat]
│        헤드 출력 결합
│ → 여러 헤드의 Attention 출력 결합
└────────────────────────────┘
               ↓
┌────────────────────────────┐
│          [Linear]
│          선형 변환
│ → 하나의 벡터로 정리하기 위해 선형 변환 적용
└────────────────────────────┘
               ↓
┌────────────────────────────┐
│       다음 레이어로 전달       
└────────────────────────────┘
```

### vs Single Attention

| Single Attention | Multi-Head Attention |
| --- | --- |
| 한 관점에서만 문맥을 파악 | 여러 관점에서 동시에 파악 |
| 단일 가중치 조합 사용 | 다양한 가중치 조합으로 시각 다양화 |
| 표현력 한계 | 표현력 확장, 정교한 의미 포착 |

## Cross-Attention

- Cross-Attention은 인코더-디코더로 구성된 트랜스포머 구조에서 디코더가 인코더의 출력(=입력 문장의 표현)을 참고하여 출력 문장을 생성하는 데 사용됩니다.
- 전에 읽은 문장, 이미지, 파일을 참고하면서 "무엇을 말해야 하지?" 하고 고민하는 과정이라 볼 수 있습니다.
- 멀티모달에서는 이미지·텍스트·음성 등 서로 다른 인코더의 출력과 디코더의 Query가 Cross-Attention으로 연결됩니다.
- Cross-Attention 또한 Multi-Head 방식으로 구성되어, 디코더는 인코더 출력을 여러 관점에서 종합적으로 참조할 수 있습니다.

> Q1 = `<BOS>` 는 디코더가 `<BOS>`를 입력받은 상태에서, 인코더 전체 출력을 참고해 첫 번째 출력을 생성하려는 Query입니다.

### 원리

```
┌───────────────────────────────┐
│          [인코더 출력]       
│ "고양이" "는" "침대" "에서" "잔다"     
└───────────────────────────────┘
               ↓                        
┌───────────────────────────────┐
│       [Key / Value 생성]          
│ "고양이" → K1, V1
│ "는"     → K2, V2
│ "침대"   → K3, V3 
│ "에서"   → K4, V4
│ "잔다"   → K5, V5              
└───────────────────────────────┘
               ↑
        [Cross-Attention]
               ↓
┌───────────────────────────────┐
│         [디코더 입력]             
│ "<BOS>" → Q1
└───────────────────────────────┘
               ↓
┌───────────────────────────────┐
│    [Query · Key 유사도 계산]        
│ Q1 ⋅ K1 →  7.2   ← "<BOS>"와 "고양이"
│ Q1 ⋅ K2 →  3.4   ← "<BOS>"와 "는"     
│ Q1 ⋅ K3 →  6.9   ← "<BOS>"와 "침대"
│ Q1 ⋅ K4 →  2.5   ← "<BOS>"와 "에서"
│ Q1 ⋅ K5 →  9.1   ← "<BOS>"와 "잔다"
└───────────────────────────────┘
               ↓
┌───────────────────────────────┐
│          [Softmax]
│ → 어텐션 스코어 (확률 분포) 변환      
└───────────────────────────────┘
               ↓
        Q1 기준 주의 분산
┌───────────────────────────────┐
│         [어텐션 스코어]                          
│ Q1 → "고양이": (0.32)              
│ Q1 → "는": (0.02)              
│ Q1 → "침대": (0.12)              
│ Q1 → "에서": (0.06)              
│ Q1 → "잔다": (0.48)                
└───────────────────────────────┘
               ↓
┌───────────────────────────────┐
│         [어텐션 가중합]       
│ (각 Value × 어텐션 스코어 합산)      
│                                      
│ Context 벡터 = ← Q1의 문맥 기반 벡터                                       
│ 0.32 × V1 +                       
│ 0.02 × V2 +                       
│ 0.12 × V3 +                       
│ 0.06 × V4 +                       
│ 0.48 × V5                         
└───────────────────────────────┘
               ↓
┌───────────────────────────────┐
│       다음 레이어로 전달            
│  (다층 트랜스포머 인코더/디코더)        
└───────────────────────────────┘
```

### vs Self-Attention

| 항목 | Self-Attention | Cross-Attention |
| --- | --- | --- |
| 사용 위치 | 인코더, 디코더 내부 | 디코더 내부 (인코더와 연결할 때) |
| Query (Q) | 같은 시퀀스에서 생성 | 디코더의 입력에서 생성 |
| Key / Value (K/V) | 같은 시퀀스에서 생성 | 인코더의 출력에서 생성 |
| 주요 목적 | 입력 내에서 문맥 관계 파악 | 입력 문장에서 필요한 정보 참조 |
| 입력 예시 | "고양이", "는", "침대" 등 → 서로 간 관계 | 디코더: "<BOS>", 인코더: "고양이", "잔다" 등 |
| Q가 주목하는 대상 | 자기 자신을 포함한 동일 시퀀스 내의 토큰들 | 다른 시퀀스(인코더 출력)의 토큰들 |
| 대표 사용처 | 인코더: 전체 입력 이해 / 디코더: 이전 단어 문맥 파악 | 디코더: 입력을 참조하여 다음 단어 생성 |
| 구성 요소 | Q, K, V 전부 같은 소스에서 생성 | Q는 디코더 / K, V는 인코더에서 생성 |
| 예시 상황 | "고양이"가 "잔다"와 어떤 관계인지 파악 | "<BOS>"가 입력 문장 중 어디를 참고해야 하는지 판단 |

### 멀티모달

- 멀티모달 모델은 텍스트뿐 아니라 이미지, 음성 등 다른 리소스를 벡터로 전환하여, 모달 간 관계를 이해합니다.
- 각 모달의 입력은 각각의 전용 인코더(비전 인코더, 오디오 인코더)를 통해 벡터로 변환된 후, 이들 벡터를 통합하여 하나의 의미 있는 표현으로 결합합니다. 
- 이미지에 대한 Cross-Attention도 일반적으로 Multi-Head 방식으로 구성되어, 여러 패치에 대한 다양한 관점에서 정보를 추출합니다.

> 여기서 인코더(Encoder)의 의미는 트랜스포머 구조에서 이야기하는 인코더만을 의미하기보다는, 조금 더 넓은 의미로 텍스트, 이미지, 오디오 등 다양한 입력을 해당 모델이 이해할 수 있는 벡터 형태로 변환하는 변환기로써, 전처리 단계의 의미입니다. 보통 특정 표현을 또 다른 형태로 변환하는 것을 인코딩(Encoding)이라고 부릅니다.

```
┌───────────────────────────────┐
│         [이미지 입력]                    
│    침대 위에 앉은 고양이 사진       
└───────────────────────────────┘
               ↓
┌───────────────────────────────┐
│          [비전 인코더]
│ - 이미지 패치를 토큰으로 분할
│ - 각 패치를 임베딩하여 시퀀스 생성
└───────────────────────────────┘
               ↓
┌───────────────────────────────┐
│    [이미지 토큰 → Key/Value]          
│ patch_1 → K1, V1
│ patch_2 → K2, V2
│ ...
│ patch_N → KN, VN
└───────────────────────────────┘
               ↑
        [Cross-Attention]
               ↓
┌───────────────────────────────┐
│         [디코더 입력]
│ Q1 → "<BOS>"
└───────────────────────────────┘
               ↓
┌───────────────────────────────┐
│    [Query · Key 유사도 계산]     
│ Q1 ⋅ K1 →  4.1  ← "<BOS>"와 patch_1     
│ Q1 ⋅ K2 →  7.3  ← "<BOS>"와 patch_2     
│ ...                                      
│ Q1 ⋅ KN →  5.9  ← "<BOS>"와 patch_N     
└───────────────────────────────┘
               ↓
┌───────────────────────────────┐
│          [Softmax]
│ → 어텐션 스코어 (확률 분포) 변환      
└───────────────────────────────┘
               ↓
┌───────────────────────────────┐
│ Context 벡터= ← "<BOS>"의 문맥 기반 벡터       
│ = Σ( attention_score × Vi )          
└───────────────────────────────┘
```

## 인코더+디코더 트랜스포머 구조

```
┌───────────────────────────────┐
│         [입력 프롬프트]
│    "안녕하세요, 오늘 날씨는?"   
└───────────────────────────────┘
               ↓
┌───────────────────────────────┐
│          [토크나이저]         
│      "안", "녕", "하", ...       
│  → [101, 345, 876, ...]      
└───────────────────────────────┘
               ↓
┌───────────────────────────────────────┐
│ 트랜스포머(Transformer)
│  
│ ┌───────────────────────────────┐    
│ │            [임베딩] 
│ │  토큰 ID → 벡터 변환              
│ │  예: "안" → [0.12, -0.5, ...]     
│ └───────────────────────────────┘    
│                ↓                        
│ ┌───────────────────────────────┐    
│ │        [포지셔널 인코딩]           
│ │ 단어 순서 정보 추가 → 임베딩 + 위치 벡터     
│ └───────────────────────────────┘   
│                ↓
│ ┌───────────────────────────────┐
│ │        [인코더 블록들 × N]                 
│ │ 각 층에서 다음 연산 반복:       
│ │           
│ │ ① Multi-Head Self-Attention                                           
│ │ ② Residual + LayerNorm                
│ │ ③ Feed Forward Network (FFN)
│ │ ④ Residual + LayerNorm             
│ └───────────────────────────────┘
│                ↓
│      ⬇ 인코더 출력 (Key/Value)
│                ↓
│ ┌───────────────────────────────┐
│ │         [디코더 입력]           
│ │ Q1 → "<BOS>"           
│ └───────────────────────────────┘
│                ↓
│ ┌───────────────────────────────┐
│ │           [임베딩]      
│ │ 정수 인덱스를 벡터로 변환     
│ │ → [[0.12, -0.5, ...], ...]   
│ └───────────────────────────────┘
│                ↓
│ ┌───────────────────────────────┐
│ │        [포지셔널 인코딩]     
│ │ 단어의 순서를 벡터에 반영
│ │ → 문맥+위치 정보 포함
│ └───────────────────────────────┘
│                ↓
│ ┌───────────────────────────────┐
│ │       [디코더 블록들 × N]                
│ │ 각 층마다 다음 연산 반복됨:                
│ │                                            
│ │ ① Masked Multi-Head Self-Attention                  
│ │ ② Residual + LayerNorm  
│ │ ③ Cross-Attention  
│ │ ④ Residual + LayerNorm  
│ │ ⑤ Feed Forward Network (FFN)  
│ │ ⑥ Residual + LayerNorm                        
│ └───────────────────────────────┘
│                ↓
│ ┌───────────────────────────────┐
│ │          [로짓 벡터]        
│ │ 어휘 집합의 각 단어에 대한 '점수' 
│ └───────────────────────────────┘
│                ↓
│ ┌───────────────────────────────┐
│ │           [Softmax]          
│ │ → Softmax로 확률 분포 생성               
│ └───────────────────────────────┘
└───────────────────────────────────────┘
                ↓
┌───────────────────────────────┐
│            [샘플링]
│ → 샘플링 전략(Top-k 등)으로 토큰 선택
│ 예: "맑" 선택됨                  
└───────────────────────────────┘
               ↓
┌───────────────────────────────┐
│          [디토크나이징]
│ 토큰 ID → 문자로 변환
│ 예: "맑", "고", "개", "임" → "맑고 개임"
└───────────────────────────────┘
```

## 어텐션, 그 이후

### Feed Forward Network

- Feed Forward Network(FFN)은 어텐션이 "고양이는 잔다와 관련 있다"고 알려줬다면, FFN은 "그럼 이 단어의 최종 의미는 '자는 고양이'로 다듬자"처럼 정제된 의미 벡터를 생성합니다.
- FFN은 각 토큰의 벡터에 대해 독립적으로 작동하는 작은 신경망으로, 어텐션에서 모은 문맥 정보를 정제하고 추상화합니다.

### Residual

- Residual은 어텐션이나 FFN의 결과만 사용하면 원래 정보가 손상될 수 있으므로 입력값(해당 디코더 블록에 들어온 입력 벡터)을 결과값에 더해서 원래 정보도 유지되도록 합니다. 

> 신경망이 너무 깊어지면 학습이 잘 안 되는 문제를 해결하기 위해 도입된 테크닉입니다.

   ```
   x₀  = 이 디코더 블록에 들어온 입력 벡터 (예: "고양이"에 해당하는 벡터)
   A   = x₀에 대해 Self-Attention 연산을 수행한 결과

   y = x₀ + A
   ```

   | 블록 종류 | 입력값 x | 연산 결과 f(x) | Residual 출력 |
   | --- | --- | --- | --- |
   | 어텐션 블록 | x (입력 임베딩) | SelfAttn(x) | x + SelfAttn(x) |
   | FFN 블록 | x | FFN(x) | x + FFN(x) |

### Layer Normalization

- FFN이나 어텐션 연산을 거치면 숫자 크기나 분포가 크게 흔들릴 수 있으므로 각 토큰의 벡터를 정규화(Normalization) 해줍니다.

## 정리

| 항목 | 설명 | 예시 |
| --- | --- | --- |
| 정의 | 단어·문장·문단 간 관계를 계산해 중요도를 반영하는 메커니즘 | "고양이"가 "잔다"와 얼마나 관련 있는지 계산 |
| Self-Attention | 하나의 문장 내에서 모든 단어들이 서로의 관계를 바라보며 가중치 계산 | Query, Key, Value → 유사도 → Softmax → 가중합 |
| Cross-Attention | 디코더가 인코더의 출력(다른 시퀀스)을 참고 | 번역: 영어 입력 → 한국어 생성 시, 영어 정보를 참고 |
| Masking | 디코더에서 미래 단어를 가려 현재까지만 참고 | Look-ahead Masking → 다음 단어 예측 시 미래 단어 차단 |
| Multi-Head Attention | 여러 관점(head)에서 관계를 동시에 계산 | Head1: 위치 중심, Head2: 형태소 중심, Head3: 문법 중심 등 |
| 주요 구성 요소 | Query(찾고 싶은 것), Key(비교 대상), Value(의미 벡터), Softmax | 도서관 비유: Q=책 찾기, K=주제 태그, V=책 내용 |
| vs Single Attention | Single: 한 관점 / Multi-Head: 여러 관점 → 더 정교한 이해 | 단어 간 다양한 관계를 종합적으로 반영 |

## 요약

- 어텐션은 단어와 단어 사이의 관계를 계산해 중요한 정보에 더 집중하게 하는 메커니즘입니다.
- Self-Attention, Cross-Attention, Multi-Head Attention이 대표적이며, Masking으로 생성 흐름 제어가 가능합니다.
- 다양한 관계를 반영해 자연스러운 문장 생성과 정확한 문맥 이해를 돕습니다.

## 생각해보기

- Self-Attention과 Cross-Attention은 어떤 점이 다르고, 각각 언제 사용되나요?
- Multi-Head Attention을 사용하는 이유는 무엇인가요?
- Masking이 없는 경우 어떤 문제가 발생할 수 있을까요?
- Query, Key, Value 각각의 역할을 도서관 비유로 설명해볼 수 있나요?
- 어텐션 메커니즘이 문맥 이해를 높이는 원리는 무엇인가요?
