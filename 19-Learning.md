# 학습

- ChatGPT, Gemini와 같은 인터넷, 책, 뉴스 등 방대한 데이터를 기반으로 사전 학습(Pre-Trained)된 언어모델에 추가적으로 새로운 지식을 학습시키는 방법입니다. 
- 실무에서는 AI의 답변 정확성을 높이는 것으로 시간을 절약하고 생산성을 개선하는 것이 목표입니다.

## 지도 학습

- 지도 학습(Supervised Learning)은 질문과 답을 함께 알려주고 따라하게 만드는 학습법입니다.
- 입력(Input)과 이에 대응하는 정답(Label)을 함께 제공하여, 모델이 입력과 출력 간의 관계를 학습하도록 하는 방식입니다.
- 정답이 있는 문제(예: 번역, 문장 분류 등)를 배우는 데 효과적입니다.
- 정답 데이터를 사람이 직접 만들어야 하므로, 라벨링 비용과 시간이 많이 들 수 있습니다.

```
[
    { 
        "prompt": "프롬프트 엔지니어링은 무엇인가요?", 
        "answer": "사용자가 원하는 대답을 제공할 수 있도록 체계적으로 "질문하는 방법"을 의미합니다"
    },
    { ... }
]
```

## 비지도 학습

- 비지도 학습(Unsupervised learning)은 데이터만 보고 스스로 패턴이나 규칙을 찾아서 학습하는 방식입니다.
- 사람이 정답을 만들어 줄 필요가 없으므로 비교적 적은 비용으로 학습합니다.

### 예시

- 다양한 뉴스 기사(정치, 스포츠, 경제 등)를 AI에게 제공하면 비슷한 주제의 뉴스끼리 분류
- 고객의 구매 데이터를 주면, 고객의 구매 패턴을 분석하여 비슷한 구매 성향을 가진 고객끼리 그룹으로 묶음

## 자기지도 학습

- 자기지도 학습(Self-supervised learning)은 문장에서 일부 단어나 토큰을 가리고, 그것을 예측(스스로 만든 문제를 푸는 방식)하는 방식으로 학습합니다.
- 사람이 정답을 만들어 줄 필요가 없으므로 비교적 적은 비용으로 학습합니다.

> GPT는 AR(Auto-Regressive Language Modeling)이라는 방식의 자기지도 학습으로 책, 웹사이트, 뉴스 같은 많은 글을 읽으며 단어와 문장 사이의 관계를 스스로 파악합니다.

### 예시

- 문장의 일부 단어를 가리고, 그것을 예측
- 문장의 앞부분을 보고 다음 단어를 예측
- 두 문장의 순서를 바꾸고, 원래 순서를 맞추게 함

## 인스트럭션 튜닝

- 인스트럭션 튜닝(Instruction Tuning)은 AI가 주어진 지시 또는 지침에 더 잘 따르도록 학습시키는 방법입니다. 
- '이런 식으로 응답하라'는 사용자 지침(instruction)을 학습시켜, 다양한 태스크에 일관된 방식으로 응답하게 만드는 기법입니다. 
- 단순히 많이 학습하는 것이 아니라, '이렇게 해줘'라는 말을 이해하고 그에 맞게 행동하는 법을 배우는 것이 핵심입니다.

> 인스트럭션 튜닝은 '모델 자체가 지시를 잘 따르도록 학습'시키는 것이고, 프롬프트 엔지니어링은 '튜닝된 모델을 잘 활용하는 방법'입니다.

> GPT는 자기지도 학습 → 인스트럭션 튜닝으로 사용자의 지시에 응답하는 방법을 배웁니다.

```
"'3살 아이'에게 상대성이론 설명해줘"

→ "상대성이론은 시공간의 곡률에 따라…" (×)
→ "음, 빛은 정말 빠르게 달리는 친구야. 그래서 시간이 느려지는 거야!" (○)
```

### 학습 방법

- 지도 학습처럼 질의/응답 명령어가 포함된 학습 데이터를 대량으로 만들어서 모델을 다시 학습시킵니다.

```
[
    { 
        "prompt": "'나는 학교에 간다' 문장을 영어로 번역해줘", 
        "answer": "I go to school."
    },
    { 
        "prompt": "다음 [뉴스]를 요약해줘. 뉴스= 프롬프트 엔지니어링은...", 
        "answer": "프롬프트 작성 방법을 최적화하는 기술입니다."
    },
    { ... }
]
```

## 강화 학습

- 강화 학습(Reinforcement Learning)은 좋은 응답에는 보상(점수), 나쁜 응답에는 벌점을 주어 점점 더 잘하도록 학습시키는 방식입니다.
- 보상을 통해 최적의 정책(Policy)을 학습하는 방식입니다.
- 평가된 다수의 데이터를 기반으로 점수 부여 기준을 선정하고 자동화된 평가(보상 모델)를 구축합니다.
- 보상 모델이 질의응답에 대해 평가하여 보상 점수를 부여합니다.
- 보상 기준이 부적절하면 AI가 잘못된 방향으로 학습될 수 있으니 주의해야 합니다.

```
Q. '프롬프트 엔지니어링' 이란?

---

A. 사용자가 원하는 대답을 제공할 수 있도록 체계적으로 '질문하는 방법'을 의미합니다. (5점)
A. 연극에서 대사나 동작을 지시하거나 상기시키기 위해 프롬프트를 작성하는 공학적 기법입니다. (0점)
```

### 인간 피드백 기반 강화 학습

- 인간 피드백 기반 강화 학습(RLHF, Reinforcement Learning with Human Feedback)은 두 가지 형태의 응답 중에서 더 선호하는 응답을 사람이 직접 선택하는 방식입니다.
- 사람이 좋아하는 응답을 기준 삼아 AI가 그 방향으로 학습하게 됩니다.
- AI 답변에 대한 사람의 피드백으로 지속적으로 답변이 개선될 수 있습니다.
- 사람의 평가가 부정확하거나 일관성이 없으면 AI의 답변도 부정확하거나 편향될 수 있습니다.
- 평가 시 정확하고 일관된 기준을 마련하는 것이 중요합니다.

> GPT는 자기지도 학습 → 인스트럭션 튜닝 → RLHF로 답변의 품질을 향상시킵니다.

#### 학습 절차

```
┌────────────────────────────────────┐
│ 지도 미세조정 (SFT, Supervised Fine-Tuning)       
│                                                     
│ - 모범 답안 데이터셋 활용                            
│ - 질문-응답 쌍 학습                                 
│ → '지시 따르기' 능력 학습 ← 인스트럭션 튜닝(Instruction Tuning)
└────────────────────────────────────┘
                    ↓
┌────────────────────────────────────────────┐
│ RLHF(Reinforcement Learning from Human Feedback)         
│                                                                      
│ ┌──────────────────────────────────────┐                                
│ │ ① 보상 모델 학습 (Reward Model Training)                         
│ │ - 하나의 질문에 여러 응답 생성                                 
│ │ - 사람 또는 AI가 선호도 평가 (예: A > B)                         
│ │ → 어떤 응답이 더 '좋은지'를 학습 (Preference-based Learning)          
│ └──────────────────────────────────────┘                                
│                   ↓
│ ┌──────────────────────────────────────┐                                
│ │ ② 정책 개선 (Policy Optimization with PPO)                      
│ │ - 보상 모델이 높게 평가한 응답을 생성하도록 학습                
│ │ - PPO 알고리즘으로 보상은 따르되 모델이 너무 급격히 바뀌지 않게 조절                                  
│ │ - 사람 기대에 부합하도록 모델 정렬 (Alignment)                   
│ └──────────────────────────────────────┘                                
└────────────────────────────────────────────┘
```

#### 요약

| 단계 | 주요 개념 | 설명 |
| --- | --- | --- |
| 지도 미세조정 | Instruction Tuning | 모범 답변을 보여주고 따라 하도록 학습 |
| 보상 모델 학습 | Preference Learning | 어떤 답이 더 좋은지 평가 기준 학습 |
| 정책 개선 | Alignment / PPO | 사람의 기대에 더 잘 맞는 방향으로 조정 |

#### vs 인스트럭션 튜닝

| 항목 | 인스트럭션 튜닝 | RLHF |
| --- | --- | --- |
| 핵심 | 지시문을 이해하고 수행하는 능력 학습 | 사람이 좋아하는 응답을 만드는 기준 학습 |
| 데이터 | 프롬프트 + 모범 답안 쌍 | 여러 응답 중 사람의 선호도 |
| 출력 목표 | 지시문을 잘 따름 | 사람이 선호할만한 스타일/내용 |
| 학습 방식 | 지도 학습 (SFT) | 강화 학습 (Reward Model + PPO) |

## GPT(디코더-only) 학습 단계

```
┌───────────────────────────────┐
│        [자기지도 학습]         
│ 웹/책/문서 등 대량 말뭉치 학습
│ → 다음 토큰 예측          
└───────────────────────────────┘
                ↓
┌───────────────────────────────┐
│      [인스트럭션 튜닝 (SFT)]
│ "지시에 따라 응답해" 훈련  ← 명시적 지시-응답 쌍으로 지도 학습 
│ → 명령어 프롬프트 처리 능력 향상 
└───────────────────────────────┘
                ↓
┌───────────────────────────────┐
│ [강화학습(RL) + 사람 피드백(RLHF)]
│ 사람 선호도 기반 점수 부여      
│ → 더 인간다운 답변 유도        
└───────────────────────────────┘
                ↓
┌───────────────────────────────┐
│         [최종 GPT 모델]         
│ 사용자 지시에 맞게 반응 + 도메인 지식 통합 가능     
└───────────────────────────────┘
```

## 정리

| 항목 | 설명 | 예시 |
| --- | --- | --- |
| 정의 | 언어모델에 새로운 지식을 학습시켜 답변 정확성을 높이는 방법 | 질의응답 데이터 학습, 지침 따르기 훈련 |
| 지도 학습 | 질문과 정답 쌍을 제공 → 따라하게 학습 | "프롬프트 엔지니어링이란?" → "질문 설계 기술입니다" |
| 비지도 학습 | 정답 없이 데이터 패턴을 스스로 학습 | 뉴스 기사 클러스터링, 고객 성향 그룹화 |
| 자기지도 학습 | 문장에서 일부 단어나 토큰을 가리고, 그것을 예측 | 문장의 앞부분을 보고 다음 단어를 예측 |
| 인스트럭션 튜닝 | '지시에 잘 따르도록' 학습 | "3살 아이에게 설명해줘" → 쉬운 표현으로 답변 |
| 강화 학습 | 보상 점수 기반 → 좋은 응답을 더 선택하도록 유도 | 잘 된 답변엔 5점, 잘못된 답변엔 0점 → 점점 개선 |
| 인간 피드백 기반 강화학습 (RLHF) | 사람이 좋은 답을 직접 선택해 평가, 선호도 반영 | "A와 B 중 어떤 답이 더 나은가?" → B 선택 → 학습 반영 |
| 학습 단계 | ① 자기지도 사전 학습 → ② 인스트럭션 튜닝 → ③ RLHF | GPT 시리즈의 전형적 학습 방식 |

## 요약

- 학습은 언어모델이 새로운 지식이나 지침을 배우도록 만드는 과정으로, 답변 품질을 높이는 핵심입니다.
- 지도 학습, 비지도 학습, 자기지도 학습, 인스트럭션 튜닝, 강화 학습 방식이 있으며, 각각 목적과 방식이 다릅니다.
- RLHF는 사람의 선호를 반영해 AI가 더 인간다운 답변을 하도록 돕습니다.

## 생각해보기

- 인스트럭션 튜닝과 일반 지도 학습은 어떤 점에서 다를까요?
- RLHF가 없으면 어떤 한계가 발생할 수 있을까요?
- 자기지도 학습은 어떤 상황에서 유리할까요?
- 사람이 평가하는 RLHF 과정에서 '일관성 없는 평가'가 문제를 일으키는 이유는 무엇인가요?
- GPT가 자기지도 학습 → 인스트럭션 튜닝 → RLHF 순서로 학습하는 이유는 무엇인가요?

---

[멀티에이전트](18-Multi-agent.md) ← 이전 / 다음 → [파인튜닝](20-Fine-tuning.md)