# BERT

- BERT(Bidirectional Encoder Representations from Transformers)는 2018년 구글(Google)이 발표한 언어 이해 모델입니다.
- 언어를 생성하는 것보다는 언어를 이해하고 분류하는 것에 초점이 맞추어져 있습니다.

## 예시

| 분야 | BERT 활용 방식 | 활용 예 |
|------|----------------|---------|
| 마케팅 | 감정 분석 | 고객 리뷰 긍/부정 분류 |
| 인사 | 문서 분류 | 이력서 자동 태깅, 고객문의 유형 분류 |
| 검색엔진 | 질의/문서 유사도 계산 | 사내 문서 자동 검색 기능 |

## 개요

- 이름에서 알 수 있듯이 보면 양방향(Bidirectional)으로 문맥을 이해하고, 트랜스포머(Transformer) 구조의 인코더(Encoder)를 사용한다는 것을 알 수 있습니다. 
- 인코더를 통해 앞뒤 모든 단어를 동시에 참고하면서 "이 단어가 문장 전체에서 어떤 역할을 하는지"를 파악합니다. 

## 인코더

```
┌───────────────────────────────┐
│          [입력 벡터]                
│   "사과" → [0.21, 0.33, ...]
└───────────────────────────────┘
               ↓
┌───────────────────────────────┐ 
│     주변 단어와의 관계 파악 
│  ("나는" ↔ "먹었다" ↔ "사과") ← Self-Attention  
│    → 관계 강도 계산 (어텐션)  
└───────────────────────────────┘
               ↓
┌───────────────────────────────┐
│    관계를 반영한 새 벡터 생성    
│  "사과" → [0.42, 0.57, ...] ← '먹었다'와 연결돼서 먹는 사과란 뜻으로 진화
└───────────────────────────────┘
               ↓
┌───────────────────────────────┐
│    다음 인코더 층으로 전달      
└───────────────────────────────┘
```

## Masked LM

- Masked LM(Masked Language Modeling)은 문장 중 일부 단어를 가리고(Masked Language Modeling) 그것을 맞추는 '빈칸 채우기' 방식으로 학습됩니다.
- 빈칸 채우기를 통해 문장의 의미 흐름을 이해하도록 훈련합니다.

```
┌───────────────────────────────┐
│         [입력 프롬프트]               
│      "나는 [MASK]을 먹었다"             
└───────────────────────────────┘
                ↓
┌───────────────────────────────┐
│           [토크나이저]                
│ "[CLS]" "나", "는", "[MASK]", "을", "먹었다" "[SEP]" 
└───────────────────────────────┘
                ↓
┌───────────────────────────────────────┐
│ 트랜스포머(Transformer)
│                           
│ ┌───────────────────────────────┐
│ │  [임베딩 + 포지셔널 + 세그먼트]    
│ │ - 각 토큰을 벡터로 변환                   
│ │ - 위치 정보 & 문장 구분 정보 포함         
│ └───────────────────────────────┘
│                ↓
│ ┌───────────────────────────────┐
│ │       [BERT 인코더 × 12]            
│ │ - Bidirectional Self-Attention           
│ │ - 문맥 기반 [MASK] 위치의 벡터 출력       
│ └───────────────────────────────┘
│                ↓
│ ┌───────────────────────────────┐
│ │           [로짓 벡터]
│ │ - 어휘 집합의 각 단어에 대한 '점수'  
│ │ - [MASK] 위치에서 로짓 벡터 생성
│ └───────────────────────────────┘
│                ↓
│ ┌───────────────────────────────┐
│ │           [Softmax]       
│ │ → Softmax로 확률 분포 생성  
│ └───────────────────────────────┘
└───────────────────────────────────────┘
                ↓
┌───────────────────────────────┐
│            [Argmax]       
│ → 확률이 가장 높은 토큰 선택
│ - 예: "사과" ← "나는 사과를 먹었다"       
└───────────────────────────────┘
```

## NSP

- NSP(Next Sentence Prediction)는 문장 간 관계를 이해하는 능력을 키우기 위해 도입된 문장 수준의 예측입니다. 
- 문장 A 다음에 문장 B가 실제로 이어지는 문장인지 판단하는 작업으로, 단어 간 관계뿐 아니라, 문장 간의 논리적 연결도 학습하게 만들기 위해 고안되었습니다.

> 최근 BERT 변형(MiniLM, RoBERTa 등)은 NSP를 제거하고 성능을 향상시켰습니다.

```
┌───────────────────────────────┐
│ 입력 문장 A: "나는 사과를 좋아한다."       
│ 입력 문장 B: "오늘은 비가 많이 온다."        
└───────────────────────────────┘
                ↓
┌───────────────────────────────┐
│        [문장 A + 문장 B]         
│ [CLS] 나는 사과를 좋아한다. [SEP] 오늘은... 
└───────────────────────────────┘
                ↓
┌───────────────────────────────┐
│          [토크나이저]                        
│ 문장을 토큰 단위로 분리 + 정수 인덱스화       
│ → [101, 8723, 903, ..., 102, 7648, 234...]   
└───────────────────────────────┘
                ↓
┌───────────────────────────────┐
│    [임베딩 + 포지셔널 인코딩]           
│   위치 정보와 토큰 임베딩 결합                
└───────────────────────────────┘
                ↓
┌───────────────────────────────┐
│        [세그먼트 임베딩]              
│ 문장 A → Segment 0 / 문장 B → Segment 1     
└───────────────────────────────┘
                ↓
┌───────────────────────────────┐
│       [BERT 인코더 × 12]         
│ Self-Attention 기반으로 문맥 학습           
└───────────────────────────────┘
                ↓
┌───────────────────────────────┐
│             [CLS]             
│ 두 문장이 연속인지 아닌지를 판단하는 대표값  
└───────────────────────────────┘
                ↓
┌───────────────────────────────┐
│             [NSP]
│ → "연속 문장이다" / "무관한 문장이다"       
└───────────────────────────────┘
```

## BERT(인코더-only) 트랜스포머 구조

```
┌───────────────────────────────┐
│         [입력 프롬프트]              
│    "나는 오늘 기분이 정말 좋다"           
└───────────────────────────────┘
                ↓
┌───────────────────────────────┐
│          [토크나이저]                   
│ → "[CLS]", "나는", "오늘", ..., "좋", "다", [SEP] 
│ → 토큰 ID로 변환: [101, 9283, 2351, ..., 2057, 102] 
└───────────────────────────────┘
                ↓
┌───────────────────────────────────────┐
│ 트랜스포머(Transformer)
│
│ ┌───────────────────────────────┐
│ │            [임베딩]                  
│ │ - 토큰 임베딩                               
│ │ - 세그먼트 임베딩 (문장 A/B 구분)           
│ │ - 포지셔널 인코딩 (위치 정보)               
│ │ → 세 가지를 더해 입력 벡터 구성             
│ └───────────────────────────────┘
│                ↓
│ ┌───────────────────────────────┐
│ │        [인코더 블록들 × N]         
│ │ - Self Attention   
│ │ 여러 층으로 반복됨            
│ └───────────────────────────────┘
│                ↓
│ ┌───────────────────────────────┐
│ │          [출력 벡터들]
│ │ - 각 토큰마다 문맥 반영된 벡터          
│ │ - [CLS] 토큰은 문장 전체 요약 정보 포함      
│ └───────────────────────────────┘
└───────────────────────────────────────┘
                ↓
┌───────────────────────────────┐
│             [Head]
│ - 예: 문장 분류, 개체명 인식           
└───────────────────────────────┘
```

## 정리

| 항목 | 설명 | 예시 |
| --- | --- | --- |
| 정의 | 2018년 구글이 발표한 '문장 이해' 특화 트랜스포머 모델 | 감정 분석, 문서 분류, 질의 응답, 유사도 계산 |
| 구조 | 트랜스포머의 인코더(Encoder)만 사용, 양방향 문맥 이해 | "나는 [MASK]를 먹었다" → '사과' 추론 |
| 학습 방식 | Masked LM (빈칸 채우기), NSP (다음 문장 예측) | "나는 [MASK]…" / "A 다음에 B가 오는가?" |
| 사용 목적 | 생성보다 이해, 분류, 문서 검색, 질의응답 등에 강점 | 이력서 자동 태깅, 리뷰 긍·부정 판별, 사내 문서 검색 |
| BERT 변형 | RoBERTa, MiniLM, DistilBERT 등 | RoBERTa: NSP 제거, MiniLM: 경량화 |
| Self-Attention 사용 | 양방향(Bidirectional) Self-Attention | 앞뒤 단어 모두 고려 |
| vs GPT | BERT: 이해 중심(인코더) / GPT: 생성 중심(디코더) | "이 문장은 긍정인가?"(BERT) / "시를 써줘"(GPT) |

## 요약

- BERT는 양방향 문맥 이해를 기반으로 문장 분류, 감정 분석, 검색 등에 특화된 트랜스포머 인코더 모델입니다.
- Masked LM과 NSP 학습 방식을 사용하여 문맥과 문장 간 논리적 연결을 학습합니다.
- 생성이 아닌 이해 중심의 모델이며, GPT와는 구조와 사용 목적이 다릅니다.

## 생각해보기

- BERT가 생성형 언어모델과 다른 점은 무엇인가요?
- Masked LM과 NSP 방식이 각각 어떤 역할을 하나요?
- 양방향 Self-Attention이 문장 이해에 어떻게 도움이 되나요?
- BERT의 변형 모델(RoBERTa, MiniLM 등)은 어떤 목적에서 사용될까요?
- 실무에서 BERT가 특히 효과적인 업무 사례는 무엇인가요?
